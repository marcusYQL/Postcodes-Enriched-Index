{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5185c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Package Importation\n",
    "\n",
    "Import statements to have the necessary packages for data cleaning (np,pd), data structuring (np,pd),\n",
    "distance calculation (geopy), and datetime calculation (datetime).\n",
    "\"\"\"\n",
    "\n",
    "# Import pandas with alias 'pd' for easy reference\n",
    "# Data importation, strucure, cleaning, and some analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Import numpy with alias 'np' for easy reference\n",
    "# Math functions and general numerical calculations\n",
    "import numpy as np\n",
    "\n",
    "# Import the distance function from geopy package\n",
    "# Calculate the distance between 2 latitude/longitude points\n",
    "import geopy.distance\n",
    "\n",
    "# Import the datetime function from datetime package\n",
    "# Convert strings to datetime variables that are able to calculate change in time, pull the day, month, year, etc, \n",
    "from datetime import datetime\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import fiona\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "import shapely.wkt\n",
    "\n",
    "import math\n",
    "\n",
    "import ast\n",
    "\n",
    "\"\"\"\n",
    "Data Importation\n",
    "\n",
    "Import the data download from Metabase query: Postcodes Enriched (All Postcodes v4) as a csv file.\n",
    "\n",
    "Cumulative added onto from Feb 2022.\n",
    "\"\"\"\n",
    "\n",
    "# Import input data from a .csv file using the pd.read_csv() function with the name of the file.\n",
    "# Note: One does not need to specify the full path of the file as long at the .ipynb file is in the same folder as the data\n",
    "# Read the table from Metabase query Postcodes Enriched (All Postcodes v4)\n",
    "post = pd.read_csv('postcodes_enriched.csv')\n",
    "post = post[['postcode']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae4576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Cleaning\n",
    "\n",
    "Remove any rows with a single missing value and filter out any blank values just in case.\n",
    "\"\"\"\n",
    "# Drop missing values from the dataset using the dropna() function\n",
    "post = post.dropna()\n",
    "# Remove any rows of data where the postcode is an empty string\n",
    "post = post[post['postcode']!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f8c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Cleaning\n",
    "\n",
    "Due to some issues where a postcode can be a unit # or not be of length 6 (Singapore standard length)\n",
    "we will have to either ignore those or remove the first character of it.\n",
    "\n",
    "There are postcodes that start with 'S', however the dataset provided by SingPost does not account for \n",
    "that so it has to be stripped out.\n",
    "\"\"\"\n",
    "# Create an empty list to add the newly cleaned/checked postcodes\n",
    "new_post = []\n",
    "\n",
    "# Loop to go through every postcode the Metabase query\n",
    "for i in post['postcode']:\n",
    "    \n",
    "    temp = str(i)\n",
    "    \n",
    "    # In the case the postcode starts with a #, remove the postcode entirely\n",
    "    if i.startswith('#'):\n",
    "        new_post.append(np.nan)\n",
    "    \n",
    "    elif i.startswith('(S)'):\n",
    "        new_post.append(temp.strip()[3:])\n",
    "        \n",
    "    # In the case the postcode starts with a (, remove the first character of the string/text\n",
    "    elif i.startswith('('):\n",
    "        new_post.append(temp.strip()[1:])\n",
    "        \n",
    "    # In the case the postcode starts with a S, remove the first character of the string/text\n",
    "    elif i.startswith('S'):\n",
    "        new_post.append(temp.strip()[1:])\n",
    "        \n",
    "    # In the case the whole string consists of 0's, remove the postcode entirely\n",
    "    elif all(c in '0' for c in i): # All 0\n",
    "        new_post.append(np.nan)\n",
    "        \n",
    "    # In the case where the length of the string is 5 after removing whitespace, when the Singapore standard postcode length is 6\n",
    "    # This usually applies for postcodes with a leading 0 which will be omitted from the data\n",
    "    # Remove the white space around the sting and add a leading 0 to it\n",
    "    elif len(i.strip()) == 5:\n",
    "        new_post.append(\"0\"+temp.strip())\n",
    "    \n",
    "    # In the case where the length of the string is 6 after removing whitespace, this matches the Singapore standard postcode length of 6\n",
    "    # Remove the white space around the sting\n",
    "    elif len(i.strip()) == 6:\n",
    "        new_post.append(temp.strip())\n",
    "        \n",
    "    # In the case where the length of the string is 7 after removing whitespace\n",
    "    # This usually applies for postcodes with some leading character\n",
    "    # Remove the whitespace around the string and remove the leading character\n",
    "    elif len(i.strip()) == 7:\n",
    "        new_post.append(temp.strip()[1:])\n",
    "    \n",
    "    # In the case where the length of the string is greater than 7 after removing whitespace\n",
    "    # Remove the postcode entirely\n",
    "    elif len(i.strip()) > 7:\n",
    "        new_post.append(np.nan)\n",
    "    \n",
    "    # In any other case, remove the postcode entirely\n",
    "    # To maintain a level of cleanliness and ensure the length of the newly created list matches the length of postv2\n",
    "    else:\n",
    "        new_post.append(np.nan)\n",
    "        \n",
    "\"\"\"\n",
    "Data Cleaning\n",
    "\n",
    "Append the cleaned postcodes to the data frame (df) and remove any that are missing.\n",
    "\n",
    "Reset the df's index so when we iterate through it with iloc it will align properly.\n",
    "\"\"\"\n",
    "\n",
    "# Create a new column to hold the cleaned postcodes\n",
    "post['postcode'] = new_post\n",
    "\n",
    "# Remove any of the new cleaned postcodes that are NA\n",
    "post = post.dropna(subset=['postcode'])\n",
    "\n",
    "post = post.sort_values(by='postcode')\n",
    "\n",
    "# Reset the index and remove the created index column to keep the dataset clean\n",
    "post = post.reset_index().drop(columns='index',axis=1)\n",
    "\n",
    "post = post.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c254ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "post[post['postcode'].str.len()==6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b5d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import postcode data with respective lat/long values\n",
    "\"\"\"\n",
    "\n",
    "sg_addresses = pd.read_csv('sg_addresses.csv',low_memory=False)\n",
    "sg_addresses = sg_addresses[['postcode','latitude','longitude']].sort_values('postcode')\n",
    "sg_addresses = sg_addresses.drop_duplicates(subset = ['postcode']).reset_index().drop(columns='index',axis=1)\n",
    "\n",
    "post2 = post.merge(sg_addresses[['postcode','latitude','longitude']], on = 'postcode', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29334fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sg_addresses[sg_addresses['postcode'].str.startswith('0')] #Both datasets can start with 0\n",
    "# post[post['postcode'].str.startswith('0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e5b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "post2 = post2.dropna(subset=['latitude'])\n",
    "post2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101992f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fiona.drvsupport.supported_drivers['kml'] = 'rw'  # enable KML support which is disabled by default\n",
    "fiona.drvsupport.supported_drivers['KML'] = 'rw'  # enable KML support which is disabled by default\n",
    "fiona.drvsupport.supported_drivers ['LIBKML'] = 'rw'  # enable KML support which is disabled by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6de402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combine lat/long into 1 column with data type POINT for route zone allocation\n",
    "\"\"\"\n",
    "postcodes_points = []\n",
    "for xy in zip(post2['longitude'], post2['latitude']):\n",
    "    postcodes_points.append(Point(xy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5e0d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_postcodes = gpd.GeoDataFrame(post2[['postcode','longitude','latitude']],\n",
    "                                 crs = {'init': 'epsg:4326'},\n",
    "                                 geometry = postcodes_points)\n",
    "geo_postcodes = geo_postcodes.reset_index().drop(columns='index',axis=1)\n",
    "geo_postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63c5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue with how polygons are store, most are stored as 'Polygon', but some are stored as 'MultiPolygon' - lat/longs are stored as a 4D\n",
    "sg = pd.read_csv('sg_zones.csv')\n",
    "sg2 = sg[['hub_id','name','short_name','polygon']]\n",
    "\n",
    "geo = []\n",
    "route_zone = []\n",
    "\n",
    "crs = 'epsg:4326'\n",
    "for i in range(0, len(sg2['polygon'])):\n",
    "    temp = sg2.iloc[i]['polygon']\n",
    "    temp2 = ast.literal_eval(temp)\n",
    "    temp3 = temp2['geometry']['coordinates']\n",
    "    \n",
    "    if all(c in '[' for c in str(temp3)[0:4]): # \n",
    "        for j in temp3:\n",
    "            temp4 = [x for td in j for x in td]\n",
    "            long = [i[0] for i in temp4]\n",
    "            lat = [i[1] for i in temp4]\n",
    "            try:   \n",
    "                poly = Polygon(zip(long, lat))\n",
    "            except:\n",
    "                continue\n",
    "            geo.append(poly)\n",
    "            route_zone.append(sg2.iloc[i]['short_name'])\n",
    "            \n",
    "    else:\n",
    "        temp4 = [x for td in temp3 for x in td]\n",
    "        long = [i[0] for i in temp4]\n",
    "        lat = [i[1] for i in temp4]\n",
    "        try:   \n",
    "            poly = Polygon(zip(long, lat))\n",
    "        except:\n",
    "            continue\n",
    "        geo.append(poly)\n",
    "        route_zone.append(sg2.iloc[i]['short_name'])\n",
    "\n",
    "\n",
    "sg_zones = gpd.GeoDataFrame(geometry=geo, crs=crs)\n",
    "sg_zones['route_zone']= route_zone\n",
    "sg_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider including conversion script to take route zones from addressing_prod_gl.zones to polygons/multipolygons\n",
    "# sg_zones = pd.read_csv('route_zones.csv')\n",
    "\n",
    "# sg_zones = sg_zones.drop(axis=1, columns=['Unnamed: 0'])\n",
    "# sg_zones['geometry'] = sg_zones['geometry'].apply(lambda x:shapely.wkt.loads(x))\n",
    "# sg_zones\n",
    "\n",
    "# sg_zones = gpd.GeoDataFrame(sg_zones[['route_zone']],\n",
    "#                             crs = {'init': 'epsg:4326'},\n",
    "#                             geometry = sg_zones['geometry'])\n",
    "# sg_zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2471e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Allocate each postcode their respective route zone\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "postcode = []\n",
    "route_zone = []\n",
    "lat = []\n",
    "long = []\n",
    "\n",
    "for i in range(0, len(geo_postcodes['geometry'])):\n",
    "    postcode_geo = geo_postcodes.iloc[i]['geometry']\n",
    "    \n",
    "    for j in range(0,len(sg_zones['route_zone'])):\n",
    "        zone_polygon = sg_zones.iloc[j]['geometry']\n",
    "        \n",
    "        if zone_polygon.contains(postcode_geo):\n",
    "            postcode.append(geo_postcodes.iloc[i]['postcode'])\n",
    "            route_zone.append(sg_zones.iloc[j]['route_zone'])\n",
    "            lat.append(geo_postcodes.iloc[i]['latitude'])\n",
    "            long.append(geo_postcodes.iloc[i]['longitude'])\n",
    "            break\n",
    "    else:\n",
    "        postcode.append(geo_postcodes.iloc[i]['postcode'])\n",
    "        route_zone.append(np.nan)\n",
    "        lat.append(geo_postcodes.iloc[i]['latitude'])\n",
    "        long.append(geo_postcodes.iloc[i]['longitude'])\n",
    "\n",
    "end = time.time()\n",
    "print( (end - start)/60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "post3 = pd.DataFrame({'postcode':postcode, 'post_lat': lat, 'post_long': long, 'route_zone': route_zone})\n",
    "post3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc92ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "post3[post3['route_zone'].str.startswith('MAINCON',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "post3[post3.postcode=='600311']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b806ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Repeat route zone allocation for FM zones\n",
    "\"\"\"\n",
    "\n",
    "fm_zones = gpd.read_file('FM.kml', driver='LIBKML')\n",
    "fm_zones2 = fm_zones[['Name','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f1433",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_zones2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_zone = []\n",
    "fm_zone_name = []\n",
    "for i in range(0,len(fm_zones2)):\n",
    "    fm_zone.append(fm_zones2.iloc[i]['Name'])\n",
    "    fm_zone_name.append(fm_zones2.iloc[i]['Name'].split('-')[0].split(' ',1)[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Find any FM zones that does not start with East/North/Central/West\n",
    "\"\"\"\n",
    "\n",
    "fm_weird_names = pd.DataFrame({'fm_zone':fm_zone, 'fm_zone_name':fm_zone_name })\n",
    "fm_weird_names2 = fm_weird_names[ (~fm_weird_names['fm_zone_name'].str.startswith('East')) \n",
    "               & (~fm_weird_names['fm_zone_name'].str.startswith('North')) \n",
    "               & (~fm_weird_names['fm_zone_name'].str.startswith('Central'))\n",
    "               & (~fm_weird_names['fm_zone_name'].str.startswith('West')) ]\n",
    "fm_weird_names2.to_csv('fm_weird_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb490cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "fm_group = []\n",
    "fm_team = []\n",
    "\n",
    "for i in range(0, len(geo_postcodes['geometry'])):\n",
    "    postcode_geo = geo_postcodes.iloc[i]['geometry']\n",
    "    \n",
    "    for j in range(0,len(fm_zones2['Name'])):\n",
    "        zone_polygon = fm_zones2.iloc[j]['geometry']\n",
    "        \n",
    "        if zone_polygon.contains(postcode_geo):\n",
    "            team = fm_zones2.iloc[j]['Name'].replace('\\t','').replace(u'\\xa0', u' ').split('-')[0].split(' ',1)[1].strip()\n",
    "            fm_team.append(team)\n",
    "            fm_group.append(team.split(' ')[0].strip())\n",
    "            break\n",
    "    else:\n",
    "        fm_team.append(np.nan)\n",
    "        fm_group.append(np.nan)\n",
    "\n",
    "end = time.time()\n",
    "print( (end - start)/60 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d8f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "post3['fm_team'] = fm_team\n",
    "post3['fm_group'] = fm_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "post3['fm_team'] = post3['fm_team'].str.upper()\n",
    "post3['fm_group'] = post3['fm_group'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950fb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post3[post3['fm_team'].str.startswith('Staff',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2d52dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c244ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Importation\n",
    "\n",
    "Import data from the previous iteration of Postcodes Enriched: postcodes_enriched_table_old.csv\n",
    "\n",
    "This dataset has the previous mappings of each postcode and its respective route zone\n",
    "\"\"\"\n",
    "\n",
    "# Read the table from previous iteration of Postcodes Enriched v5\n",
    "old_route_zones = pd.read_csv('postcodes_enriched_table_old.csv')\n",
    "\n",
    "# Extract the key variables of interest: previous postcode (key) and route zone\n",
    "prev_route_zones = old_route_zones[['postcode','route_zone']]\n",
    "\n",
    "prev_route_zones['postcode'] = prev_route_zones['postcode'].astype(str)\n",
    "\n",
    "prev_route_zones\n",
    "# Creating a necessary lists to hold information and build the legacy route zone\n",
    "new_post2 = []\n",
    "\n",
    "# Loop to go through every postcode of the previous iteration of postcodes enriched\n",
    "for i in prev_route_zones['postcode']:\n",
    "    \n",
    "    # Same cleaning step as before\n",
    "    # Will need to be modified if changes are made above\n",
    "    temp = str(i)\n",
    "    if temp.startswith('#'):\n",
    "        new_post2.append(np.nan)\n",
    "    elif temp.startswith('('):\n",
    "        new_post2.append(temp.strip()[1:])\n",
    "    elif temp.startswith('S'):\n",
    "        new_post2.append(temp.strip()[1:])\n",
    "    elif all(c in '0' for c in temp): # All 0\n",
    "        new_post2.append(np.nan)\n",
    "    elif len(temp) == 5:\n",
    "        new_post2.append(\"0\"+temp.strip())\n",
    "    elif len(temp) == 6:\n",
    "        new_post2.append(temp.strip())\n",
    "    elif len(temp) == 7:\n",
    "        new_post2.append(temp.strip()[1:])\n",
    "    elif len(temp) > 7:\n",
    "        new_post2.append(np.nan)\n",
    "    else:\n",
    "        new_post2.append(np.nan)\n",
    "\n",
    "# Create new variables to hold the cleaned legacy postcodes\n",
    "prev_route_zones['postcode'] = new_post2\n",
    "\n",
    "# # Remove the previous 'uncleaned' postcodes and rename the newly cleaned postcodes \n",
    "prev_route_zones = prev_route_zones.rename(columns={\"route_zone\":\"prev_route_zone\"})\n",
    "\n",
    "# # Merge/Left join the data together, adding the previous route zone\n",
    "post4 = post3.merge(prev_route_zones,on='postcode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89fb062",
   "metadata": {},
   "outputs": [],
   "source": [
    "post4['route_zone'] = post4['route_zone'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80142459",
   "metadata": {},
   "outputs": [],
   "source": [
    "post4[post4.postcode=='600311']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f04ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "post4 = post4.drop_duplicates(subset=['postcode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a necessary lists to hold information and build the current route zone\n",
    "subzone = []\n",
    "\n",
    "# Loop to go through every current route zone\n",
    "for i in post4['route_zone']:\n",
    "    \n",
    "    if i == 'nan':\n",
    "        subzone.append(np.nan)\n",
    "        continue\n",
    "        \n",
    "    # Extract the subzone from each route zone by taking the evrything before the first -\n",
    "    subzone.append(i.split('-')[0].strip())\n",
    "    \n",
    "\n",
    "# Create new variables to hold the current subzone\n",
    "post4['subzone'] = subzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "\n",
    "\"\"\"\n",
    "Data Importation\n",
    "\n",
    "Import the data download from Metabase: Postcodes Enriched (Hub's Route Zones v4).\n",
    "\n",
    "This dataset has the mappings of each route_zone to its proper hub.\n",
    "\n",
    "If a hub gets 'turned off', route_zones under it should be updated and redirected to another hub.\n",
    "\"\"\"\n",
    "station_route_zones = pd.read_csv('station_route_zones.csv')\n",
    "# station_route_zones = pd.read_csv('station_route_zones_temp.csv')\n",
    "\n",
    "# Remove duplicate route zones just in case as 1 route zone can only belong to 1 station/hub\n",
    "station_route_zones = station_route_zones.drop_duplicates('route_zone')\n",
    "\n",
    "\"\"\"\n",
    "Data Joining\n",
    "\n",
    "Join the 2 previous datasets to postcodes.\n",
    "\"\"\"\n",
    "# Merge/Left join the data together, adding the proper destination station to each route zone\n",
    "post5 = post4.merge(station_route_zones, on='route_zone',how='left')\n",
    "\n",
    "# Rename the columns to something more descriptive\n",
    "post5 = post5.rename(columns={'station_name':'dest_station', 'station_id':'dest_station_id'})\n",
    "\n",
    "# Drop additional columns\n",
    "# post5 = post5.drop(columns=['name','route_zone_id'])\n",
    "post5 = post5.drop(columns=['route_zone_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "post5[post5.postcode=='600311']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879bc8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Data Creation\n",
    "\n",
    "Calculate the distance between each postal code to its respective station.\n",
    "\"\"\"\n",
    "\n",
    "# Creating a necessary lists to hold information and build the distance from the centroid of a route zone to its destination station\n",
    "dist_station_post = []\n",
    "\n",
    "\n",
    "# Loop to go through every postcode's route zone centroid\n",
    "for i in post5['postcode']:\n",
    "    \n",
    "    temp = post5[post5.postcode == i]\n",
    "#     print(temp['post_lat'].values[0])\n",
    "    \n",
    "    # Check if any of the key variables in the calculation is null/NaN/missing, if any are missing then skip over this postcode\n",
    "    if temp.isnull().values.any():\n",
    "        dist_station_post.append(np.nan)\n",
    "        continue\n",
    "    else:\n",
    "    # Save each variable separately\n",
    "        post_lat = temp['post_lat'].values[0]\n",
    "        post_long = temp['post_long'].values[0]\n",
    "        station_lat = temp['station_lat'].values[0]\n",
    "        station_long = temp['station_long'].values[0]\n",
    "    \n",
    "    # Calculate the distance from the centroid of a route zone to its destination station\n",
    "    dist_station_post.append(geopy.distance.distance( (post_lat,post_long),(station_lat,station_long) ).km)\n",
    "\n",
    "# Create new variables to hold the distance calculated above\n",
    "post5['dist_from_station_to_post'] = dist_station_post\n",
    "\n",
    "# post5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "post5[post5.postcode=='600311']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Importation\n",
    "\n",
    "Import the data download from Metabase: Postcodes Enriched(Parcel volume of postcode v4)\n",
    "\n",
    "This dataset has the both doorstep, PUDO, and RPU orders for all postcodes within the previous month.\n",
    "\n",
    "Import the data download from Redash: calendar.csv\n",
    "\n",
    "19th of the previous month to the 19th of the current month.\n",
    "\"\"\"\n",
    "postcode_volume = pd.read_csv('postcodes_volume.csv')\n",
    "postcode_volume = postcode_volume.dropna()\n",
    "calendar = pd.read_csv('calendar.csv') # Update for next year\n",
    "\n",
    "new_post3 = []\n",
    "\n",
    "# Loop through every postcode with 'recent' (19th to 19th) volume\n",
    "for i in postcode_volume['postcode']:\n",
    "    \n",
    "    # Same cleaning step as before\n",
    "    # Will need to be modified if changes are made above\n",
    "    temp = str(i)\n",
    "    if temp.startswith('#'):\n",
    "        new_post3.append(np.nan)\n",
    "    elif temp.startswith('S'):\n",
    "        new_post3.append(temp.strip()[1:])\n",
    "    elif temp.startswith('('):\n",
    "        new_post3.append(temp.strip()[1:])\n",
    "    elif all(c in '0' for c in temp): # All 0\n",
    "        new_post3.append(np.nan)\n",
    "    elif len(temp) == 5:\n",
    "        new_post3.append(\"0\"+temp.strip())\n",
    "    elif len(temp) == 6:\n",
    "        new_post3.append(temp.strip())\n",
    "    elif len(temp) == 7:\n",
    "        new_post3.append(temp.strip()[1:])\n",
    "    elif len(temp) > 7:\n",
    "        new_post3.append(np.nan)\n",
    "    else:\n",
    "        new_post3.append(np.nan)\n",
    "\n",
    "\"\"\"\n",
    "Data Creation\n",
    "\n",
    "Calculate the volume for each postcode and the average volume (total/working_days) using volume and no of working days.\n",
    "\"\"\"\n",
    "\n",
    "# Create a new variable to hold the 'monthly' volume\n",
    "postcode_volume['postcode']=new_post3\n",
    "postcode_volume2 = postcode_volume.groupby('postcode').sum()\n",
    "\n",
    "# Merge/Left join the data together, adding monthly volume for each postcode\n",
    "post6 = post5.merge(postcode_volume2,on='postcode',how='left')\n",
    "\n",
    "# Rename certain variables to be more descriptive\n",
    "post6 = post6.rename(columns={'volume':'monthly_volume'})\n",
    "\n",
    "# Get the current date's month\n",
    "current_date = datetime.today().strftime('%Y-%m')\n",
    "date_before_month = datetime.today() - relativedelta(months=1)\n",
    "\n",
    "\n",
    "# Calculate the number of working days between the 19th to 19th\n",
    "no_working_days = calendar[calendar['date']==current_date+'-19']['working_day_cum'].values[0] - calendar[calendar['date']== date_before_month.strftime('%Y-%m')+'-19']['working_day_cum'].values[0]\n",
    "\n",
    "# Calculate the average daily volume: monthly volume / no working days and create a new variable for it\n",
    "post6['avg_daily_volume'] = round(post6['monthly_volume']/no_working_days,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "post6[post6.postcode=='134028']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Importation\n",
    "\n",
    "Import the data download from Fleet: FM.csv\n",
    "\n",
    "This dataset shows the mapping of each subzone to WEST, CENTRAL, NORTH, EAST First Mile (FM) groups, teams and \n",
    "the proper dropoff hub (CK).\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Data Importation\n",
    "\n",
    "Import the data download from Fleet: hubs.csv\n",
    "\n",
    "This dataset shows the lat and long of hubs PANDAN and YCK.\n",
    "\"\"\"\n",
    "\n",
    "main_hubs = pd.read_csv('hubs.csv')\n",
    "\n",
    "post7 = post6\n",
    "\n",
    "post7 = post7.drop_duplicates(subset='postcode')\n",
    "\n",
    "post7['fm_dropoff_hub'] = 'YCK'\n",
    "\n",
    "# Merge/Left join the data together, adding dropoff hub information (lat/long)\n",
    "post8 = post7.merge(main_hubs,on='fm_dropoff_hub',how='left')\n",
    "\n",
    "# Remove unused data\n",
    "post8 = post8.drop(columns=['hub_id'])\n",
    "\n",
    "post8['fm_group'] = post8['fm_group'].str.upper()\n",
    "post8['fm_team'] = post8['fm_team'].str.upper()\n",
    "post8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2479558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 69 postocdes could not be mapped, J08-F, K06-G, K06-E, G01-G, F10-D\n",
    "# post8[post8.fm_team.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aacf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "post8[post8.postcode=='600311']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8ba1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = post8\n",
    "\n",
    "count = 0\n",
    "\n",
    "for i in range(0, len(temp['fm_group'])):\n",
    "    temp2 = temp.iloc[0]\n",
    "    group = temp2['fm_group']\n",
    "    team = temp2['fm_team'].split(' ')[0]\n",
    "    \n",
    "    if group != team:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b150574e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize names and add a checker here to inform Fel if any teams/groups are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852db9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Creation\n",
    "\n",
    "Calculate the distance between each postcode to their respective FM dropoff hub (PANDAN or YCK).\n",
    "\"\"\"\n",
    "\n",
    "# Creating a necessary lists to hold information and build distance from each postcode to dropoff hub\n",
    "dist_dropoff_hub = []\n",
    "\n",
    "# Loop through every postcode's dropoff hub\n",
    "for i in range(0,len(post8['fm_dropoff_hub_latitude'])):\n",
    "    temp = post8.iloc[i]\n",
    "    \n",
    "    # Check if any of the key variables in the calculation is null/NaN/missing, if any are missing then skip over this postcode\n",
    "    if np.isnan(temp['post_lat']) or np.isnan(temp['post_long']) or np.isnan(temp['fm_dropoff_hub_latitude']) or np.isnan(temp['fm_dropoff_hub_longitude']):\n",
    "        dist_dropoff_hub.append(np.nan)\n",
    "        continue\n",
    "    else:\n",
    "        # Save each variable separately\n",
    "        post_lat = temp['post_lat']\n",
    "        post_long = temp['post_long']\n",
    "        main_hub_lat = temp['fm_dropoff_hub_latitude']\n",
    "        main_hub_long = temp['fm_dropoff_hub_longitude']\n",
    "    \n",
    "    # Calculate the distance from the centroid of a route zone to its destination station\n",
    "    dist_dropoff_hub.append(geopy.distance.distance( (post_lat,post_long),(main_hub_lat,main_hub_long) ).km)\n",
    "\n",
    "# Create new variables to hold the distance from each postcode to dropoff hub\n",
    "post8['dist_from_post_to_dropoff_hub'] = dist_dropoff_hub\n",
    "\n",
    "post8 = post8.drop(columns=['fm_dropoff_hub_latitude','fm_dropoff_hub_longitude'])\n",
    "\n",
    "post8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ec2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "post8[post8.postcode=='600311']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcea44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Importation\n",
    "\n",
    "Import the data download from OPEX: restricted.csv\n",
    "\n",
    "This dataset shows all the postcodes we 'should not' be able to service. But there are some we do service anyway.\n",
    "\"\"\"\n",
    "# Will have to check in with OPEX every quarter or so to check if this list has changed\n",
    "restricted = pd.read_csv('restricted.csv')\n",
    "new_restricted_post = []\n",
    "\n",
    "\"\"\"\n",
    "Data Cleaning\n",
    "\n",
    "Code here is a bit too much as the main issue is that postcodes that start with '0' loses the '0' so it adds it back.\n",
    "\"\"\"\n",
    "\n",
    "# Loop through every restricted postcode\n",
    "for i in restricted['Postal Code']:\n",
    "    \n",
    "    # Same cleaning step as before\n",
    "    # Will need to be modified if changes are made above\n",
    "    temp = str(i)\n",
    "    if temp.startswith('#'):\n",
    "        new_restricted_post.append(np.nan)\n",
    "    elif temp.startswith('S'):\n",
    "        new_restricted_post.append(temp[1:])\n",
    "    elif temp.startswith('('):\n",
    "        new_restricted_post.append(temp[1:])\n",
    "    elif all(c in '0' for c in temp): # All 0\n",
    "        new_restricted_post.append(np.nan)\n",
    "    elif len(temp) == 5:\n",
    "        new_restricted_post.append(\"0\"+temp)\n",
    "    elif len(temp) == 6:\n",
    "        new_restricted_post.append(temp)\n",
    "    elif len(temp) == 7:\n",
    "        new_restricted_post.append(temp[1:])\n",
    "    elif len(temp) > 7:\n",
    "        new_restricted_post.append(np.nan)\n",
    "    else:\n",
    "        new_restricted_post.append(np.nan)\n",
    "\n",
    "\"\"\"\n",
    "Data Creation\n",
    "\n",
    "If the postal code is restricted then 1 else 0. This will act as a flag to indicate which postcodes are restricted.\n",
    "\"\"\"\n",
    "# Creating a necessary lists to hold information and build which postcodes are restricted\n",
    "restricted_flag = []\n",
    "\n",
    "# Loop through every postcode in the base dataset\n",
    "for i in post8['postcode']:\n",
    "    \n",
    "    # In the case the postcode matches one in the restricted list then flag as 1\n",
    "    if i in np.array(new_restricted_post):\n",
    "        restricted_flag.append(1)\n",
    "    # In the case it is not, flag as 0\n",
    "    else:\n",
    "        restricted_flag.append(0)\n",
    "\n",
    "# Create new variables to hold the restricted flag\n",
    "post8['restricted_zone_flag'] = restricted_flag\n",
    "post8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b9cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post8[post8.postcode=='018895']\n",
    "post8[post8.postcode=='600311']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65294a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Importation\n",
    "\n",
    "Import the data download from SingPost: 6D.csv\n",
    "\n",
    "This dataset has the full list of postcodes in Singapore and their building_type.\n",
    "\"\"\"\n",
    "# This is the base dataset from SingPost that we will add to as it is a 'more' encompassing dataset\n",
    "post6D = pd.read_csv('6D.csv')\n",
    "\n",
    "# Creating a necessary lists to hold information and build the base postcodes dataset\n",
    "new_post4 = []\n",
    "\n",
    "\"\"\"\n",
    "Data Cleaning\n",
    "\n",
    "Code here is a bit too much as the main issue is that postcodes that start with '0' loses the '0' so it adds it back.\n",
    "\"\"\"      \n",
    "# Loop to go through every 6D postcode\n",
    "for i in post6D['POSTCODE']:\n",
    "    \n",
    "    # Same cleaning step as before\n",
    "    # Will need to be modified if changes are made above\n",
    "    temp = str(i)\n",
    "    if temp.startswith('#'):\n",
    "        new_post4.append(np.nan)\n",
    "    elif temp.startswith('S'):\n",
    "        new_post4.append(temp.strip()[1:])\n",
    "    elif temp.startswith('('):\n",
    "        new_post4.append(temp.strip()[1:])\n",
    "    elif all(c in '0' for c in temp): # All 0\n",
    "        new_post4.append(np.nan)\n",
    "    elif len(temp) == 5:\n",
    "        new_post4.append(\"0\"+temp.strip())\n",
    "    elif len(temp) == 6:\n",
    "        new_post4.append(temp.strip())\n",
    "    elif len(temp) == 7:\n",
    "        new_post4.append(temp.strip()[1:])\n",
    "    elif len(temp) > 7:\n",
    "        new_post4.append(np.nan)\n",
    "    else:\n",
    "        new_post4.append(np.nan)\n",
    "\n",
    "# Create new variables to hold the cleaned 6D postcodes\n",
    "post6D['POSTCODE'] = new_post4\n",
    "\n",
    "\"\"\"\n",
    "Data Relabelling\n",
    "\n",
    "To simplify the building codes from 8 categories and 66 codes into 1 category and 15 codes.\n",
    "\n",
    "Combine similiar building types like Condo and Condo with Retail.\n",
    "\"\"\"\n",
    "# Creating a necessary lists to hold information and build the postcode building type\n",
    "new_bldgcode = []\n",
    "\n",
    "# Extract the building types from the 6D dataset\n",
    "bldgcode = post6D['BLDGCODE']\n",
    "\n",
    "# Loop to go through every building type designation\n",
    "for i in bldgcode:\n",
    "    \n",
    "    # Relabelling SingPost's building type designations to our own X# code\n",
    "    # For a more comprehensive breakdown, refer below and to ...\n",
    "    if str(i) == 'C03' or str(i) == 'C11' or str(i) == 'T01' or str(i) == 'T02':\n",
    "        new_bldgcode.append(\"X01\") #C03\n",
    "        \n",
    "    elif str(i) == 'C04' or str(i) == 'C01' or str(i) == 'C02' or str(i) == 'C14':\n",
    "        new_bldgcode.append(\"X02\") #C04\n",
    "        \n",
    "    elif str(i) == 'C06' or str(i) == 'C05' or str(i) == 'C07':\n",
    "        new_bldgcode.append(\"X03\") #C06\n",
    "        \n",
    "    elif str(i) == 'C08':\n",
    "        new_bldgcode.append(\"X04\")\n",
    "        \n",
    "    elif str(i) == 'C09' or str(i) == 'C10':\n",
    "        new_bldgcode.append(\"X05\")\n",
    "        \n",
    "    elif str(i) == 'C12' or str(i) == 'C13' or str(i) == 'T03' or str(i) == 'T04':\n",
    "        new_bldgcode.append(\"X06\")\n",
    "    \n",
    "    elif str(i) == 'R01':\n",
    "        new_bldgcode.append(\"X07\")\n",
    "    \n",
    "    elif str(i) == 'R02' or str(i) == 'R03' or str(i) == 'R06':\n",
    "        new_bldgcode.append(\"X08\")\n",
    "    \n",
    "    elif str(i) == 'R04' or str(i) == 'R05':\n",
    "        new_bldgcode.append(\"X09\")\n",
    "        \n",
    "    elif str(i) == 'E01' or str(i) == 'E02' or str(i) == 'E03' or str(i) == 'E06':\n",
    "        new_bldgcode.append(\"X10\")\n",
    "    \n",
    "    elif str(i) == 'E04' or str(i) == 'E05' or str(i) == 'E07':\n",
    "        new_bldgcode.append(\"X11\")\n",
    "        \n",
    "    elif str(i).startswith(\"L\"):\n",
    "        new_bldgcode.append(\"X12\")\n",
    "    \n",
    "    elif str(i).startswith(\"H\"):\n",
    "        new_bldgcode.append(\"X13\")\n",
    "    \n",
    "    elif str(i).startswith(\"W\"):\n",
    "        new_bldgcode.append(\"X14\")\n",
    "        \n",
    "    elif str(i).startswith(\"Z\"):\n",
    "        new_bldgcode.append(\"X15\")\n",
    "    \n",
    "    else:\n",
    "        new_bldgcode.append(np.nan)\n",
    "        \n",
    "# Create new variables to hold the relabelled building type codes\n",
    "post6D['new_bldgcode'] = new_bldgcode\n",
    "\n",
    "# Drop duplicate cleaned postcodes (Very unlikely, but just in case)\n",
    "post6D = post6D.drop_duplicates('POSTCODE')\n",
    "post6D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2e7838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Creation\n",
    "\n",
    "With the new building_code established (X01 - X15), label each with a building_type.\n",
    "\"\"\"\n",
    "# Creating a necessary lists to hold information and build the building type name\n",
    "bldg_type = []\n",
    "\n",
    "# Loop through every building type code of each postcode\n",
    "for i in post6D['new_bldgcode']:\n",
    "    \n",
    "    # Label each building type code with its proper name\n",
    "    if i == \"X01\":\n",
    "        bldg_type.append(\"Shophouse/Walkup\")\n",
    "    elif i == \"X02\":\n",
    "        bldg_type.append(\"Industrial/Factory/Warehouse\")\n",
    "    elif i == \"X03\":\n",
    "        bldg_type.append(\"Office\")\n",
    "    elif i == \"X04\":\n",
    "        bldg_type.append(\"Shopping/Dining\")\n",
    "    elif i == \"X05\":\n",
    "        bldg_type.append(\"Hotel/Hostel/Works_Dorms\")\n",
    "    elif i == \"X06\":\n",
    "        bldg_type.append(\"Port/Shipyard\")\n",
    "    elif i == \"X07\":\n",
    "        bldg_type.append(\"HDB\")\n",
    "    elif i == \"X08\":\n",
    "        bldg_type.append(\"Condo\")\n",
    "    elif i == \"X09\":\n",
    "        bldg_type.append(\"Landed\")\n",
    "    elif i == \"X10\":\n",
    "        bldg_type.append(\"Educational_Institute\")\n",
    "    elif i == \"X11\":\n",
    "        bldg_type.append(\"Higher_Education\")\n",
    "    elif i == \"X12\":\n",
    "        bldg_type.append(\"Community_Club\")\n",
    "    elif i == \"X13\":\n",
    "        bldg_type.append(\"Medical_Facility\")\n",
    "    elif i == \"X14\":\n",
    "        bldg_type.append(\"Religious_Organization\")\n",
    "    elif i == \"X15\":\n",
    "        bldg_type.append(\"Misc_Buildings\")\n",
    "    else:\n",
    "        bldg_type.append(np.nan)\n",
    "        \n",
    "# Create new variables to hold the building type name\n",
    "post6D['bldg_type'] = bldg_type\n",
    "\n",
    "post6D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Joining\n",
    "\n",
    "Join the 6D dataset to postcodes and standardize the columns names.\n",
    "\"\"\"\n",
    "# Merge/Left join the data together, using the 6D dataset as a base and joining our previously calculated values to it\n",
    "post9 = post6D.merge(post8, left_on='POSTCODE', right_on=\"postcode\",how='left')\n",
    "\n",
    "# Remove old variables\n",
    "post9 = post9.drop(columns=['postcode', 'BLDGNO', 'STREETNAME', 'BLDGNAME',\n",
    "                              'BLDGNAME', 'BLDGCODE', 'BLDGDESC'])\n",
    "\n",
    "# Rename certain variables to be more in tune with other variables\n",
    "post9 = post9.rename(columns={'new_bldgcode':'bldg_code', 'POSTCODE':'postcode'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "post9[post9.postcode=='600311']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import Postcodes Index\n",
    "\"\"\"\n",
    "\n",
    "PI = pd.read_csv('PI_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764e139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a necessary lists to hold information and build the base postcodes dataset\n",
    "new_post5 = []\n",
    "\n",
    "\"\"\"\n",
    "Data Cleaning\n",
    "\n",
    "Code here is a bit too much as the main issue is that postcodes that start with '0' loses the '0' so it adds it back.\n",
    "\"\"\"      \n",
    "# Loop to go through every 6D postcode\n",
    "for i in PI['postcode']:\n",
    "    \n",
    "    # Same cleaning step as before\n",
    "    # Will need to be modified if changes are made above\n",
    "    temp = str(i)\n",
    "    if temp.startswith('#'):\n",
    "        new_post5.append(np.nan)\n",
    "    elif temp.startswith('S'):\n",
    "        new_post5.append(temp.strip()[1:])\n",
    "    elif temp.startswith('('):\n",
    "        new_post5.append(temp.strip()[1:])\n",
    "    elif all(c in '0' for c in temp): # All 0\n",
    "        new_post5.append(np.nan)\n",
    "    elif len(temp) == 5:\n",
    "        new_post5.append(\"0\"+temp.strip())\n",
    "    elif len(temp) == 6:\n",
    "        new_post5.append(temp.strip())\n",
    "    elif len(temp) == 7:\n",
    "        new_post5.append(temp.strip()[1:])\n",
    "    elif len(temp) > 7:\n",
    "        new_post5.append(np.nan)\n",
    "    else:\n",
    "        new_post5.append(np.nan)\n",
    "\n",
    "# Create new variables to hold the \n",
    "PI['postcode'] = new_post5\n",
    "PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1085cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "post10 = post9.merge(PI[['postcode','pi_within','pi_across_short','pi_across_long']], on='postcode', how='left')\n",
    "post10 = post10.rename(columns={'pi_within':'difficulty_within','pi_across_short':'difficulty_across_short','pi_across_long':'difficulty_across_long'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09a62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4544f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Creation\n",
    "\n",
    "Created and expiry dates are used to keep track of which version of the table we are looking at.\n",
    "\n",
    "When a 6 month rolling window (6 months of data) is keep, we can use the 2 dates to backtrack.\n",
    "\"\"\"\n",
    "\n",
    "current_date = datetime.today().strftime('%m/%y')\n",
    "\n",
    "date_after_month = datetime.today()+ relativedelta(months=1)\n",
    "\n",
    "# Timestamp of when the table was roughly created and roughly when it will be outdated\n",
    "# 19th of the current month\n",
    "post10['created_on'] = '19/'+ current_date\n",
    "\n",
    "\n",
    "# 19th of the following month\n",
    "# At the end of the every year, must switch to the following year\n",
    "post10['expiry_on'] = '19/'+ date_after_month.strftime('%m/%y')\n",
    "\n",
    "post10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe97188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import districts of SG data and match the first 2 digits of each postcode to a district\n",
    "\"\"\"\n",
    "\n",
    "districts = pd.read_csv('districts.csv')\n",
    "new_post6 = []\n",
    "for i in districts['postcode sector']:\n",
    "    if len( str(i) ) == 1:\n",
    "        new_post6.append('0' + str(i))\n",
    "    else:\n",
    "        new_post6.append(str(i))\n",
    "districts['leading_post'] = new_post6\n",
    "\n",
    "post10['temp_postcode'] = post10['postcode'].str[0:2]\n",
    "\n",
    "post11 = post10.merge(districts[['leading_post','state']], left_on='temp_postcode', right_on='leading_post', how='left')\n",
    "post11 = post11.drop(['leading_post','temp_postcode'],axis=1)\n",
    "post11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db65953",
   "metadata": {},
   "outputs": [],
   "source": [
    "post12 = post11[['postcode','post_lat', 'post_long','bldg_type','route_zone',\n",
    "                 'subzone','prev_route_zone','dest_station_id','dest_station', 'station_lat', 'station_long',\n",
    "                 'dist_from_station_to_post','monthly_volume','avg_daily_volume',\n",
    "                 'fm_group','fm_team','fm_dropoff_hub','dist_from_post_to_dropoff_hub','restricted_zone_flag',\n",
    "                 'difficulty_within','difficulty_across_short','difficulty_across_long','state','created_on','expiry_on']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Export\n",
    "\"\"\"\n",
    "\n",
    "# Add a month year timestamp for easy distinction of iterations of Postcode Enriched\n",
    "timestamp = datetime.today().strftime(\"%B_%Y\")\n",
    "\n",
    "# Table export\n",
    "# post12.to_csv('postcodes_enriched_table_v5.5_'+timestamp+'.csv')\n",
    "post12.to_csv('postcodes_enriched_table_v5.5_'+timestamp+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f2627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "post12 = pd.read_csv('postcodes_enriched_table_v5.5_'+timestamp+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2e9d0-4f03-4acd-84e2-00d6609e9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adjust postcodes difficulty data for upload to Metabase\n",
    "\"\"\"\n",
    "\n",
    "post13 = post12.copy()\n",
    "post13['difficulty_across'] = (post13['difficulty_across_short'] + post13['difficulty_across_long'])/2\n",
    "post13 = post13.drop(axis=1,columns=['difficulty_across_short','difficulty_across_long'])\n",
    "\n",
    "post14 = post13[['postcode','post_lat', 'post_long','bldg_type','route_zone',\n",
    "                 'subzone','prev_route_zone','dest_station_id','dest_station', 'station_lat', 'station_long',\n",
    "                 'dist_from_station_to_post','monthly_volume','avg_daily_volume',\n",
    "                 'fm_group','fm_team','fm_dropoff_hub','dist_from_post_to_dropoff_hub','restricted_zone_flag',\n",
    "                 'difficulty_within','difficulty_across','state','created_on','expiry_on']]\n",
    "post14.to_csv('postcodes_enriched_table_v5.5_'+timestamp+'_to_upload.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3660a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "post12[post12.postcode==134028]['monthly_volume']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfbc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# post12[post12.route_zone.str.contains('(25/2)',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bldg = post12.groupby('bldg_type')['monthly_volume'].sum()\n",
    "bldg2 = bldg.reset_index()\n",
    "bldg2.to_csv('bldg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb34dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "station = post12.groupby('dest_station')['monthly_volume'].sum()\n",
    "station2 = station.reset_index()\n",
    "station2.to_csv('station.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2765b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_zone = post12.groupby('route_zone')['monthly_volume'].sum()\n",
    "route_zone2 = route_zone.reset_index()\n",
    "route_zone2.to_csv('route_zone.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e2b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "route_zone_bldg = post12.groupby(['route_zone','bldg_type'])['monthly_volume'].sum()\n",
    "route_zone_bldg2 = route_zone_bldg.reset_index()\n",
    "route_zone_bldg2.to_csv('route_zone_bldg.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bd978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "post12.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346b6ae4-48d5-48de-9d11-53d753d94650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008322d-e584-459c-b52b-934011626595",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
